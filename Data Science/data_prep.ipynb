{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Prep and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qt/r87857tx74n48x9wj7f9wljw0000gn/T/ipykernel_66199/2550901138.py:12: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  weather_df['timestamp'] = pd.to_datetime(weather_df['timestamp']).astype(int) // 10**9\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "leak_data_raw =  pd.read_csv(\"Gasleak Data Sets/sensor_readings.csv\")\n",
    "leak_data_raw.columns = [\"temp\",\"Time\",\"Sensor 1\", \"Sensor 2\", \"Sensor 3\", \"Sensor 4\", \"Sensor 5\", \"Sensor 6\", \"Sensor 7\", \"Sensor 8\", \"Sensor 9\", \"Sensor 10\", \"Sensor 11\", \"Sensor 12\", \"Sensor 13\", \"Sensor 14\", \"Sensor 15\", \"Sensor 16\", \"Sensor 17\", \"Sensor 18\", \"Sensor 19\", \"Sensor 20\", \"Sensor 21\", \"Sensor 22\", \"Sensor 23\", \"Sensor 24\"]\n",
    "\n",
    "weather_df = pd.read_csv(\"Gasleak Data Sets/weather_data.csv\")\n",
    "columns_to_average = ['Barometric_Pressure', 'Humidity', 'Temperature', 'Wind_Direction', 'Wind_Speed']\n",
    "for col in columns_to_average:\n",
    "    weather_df[col] = weather_df.groupby('timestamp')[col].transform('mean')\n",
    "weather_df = weather_df.drop_duplicates()\n",
    "weather_df['timestamp'] = pd.to_datetime(weather_df['timestamp']).astype(int) // 10**9\n",
    "weather_df = weather_df.rename(columns={'timestamp': 'tTime'})\n",
    "\n",
    "leak_rate_df = pd.read_csv(\"Gasleak Data Sets/leak_locations_and_rate.csv\")\n",
    "leak_rate_df =  leak_rate_df[['LeakRate', 'tStart', 'tEnd', 'Duration']]\n",
    "leak_rate_df['LeakRate'] = leak_rate_df.groupby('tStart')['LeakRate'].transform('mean')\n",
    "leak_rate_df = leak_rate_df.drop_duplicates()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/qt/r87857tx74n48x9wj7f9wljw0000gn/T/ipykernel_66199/4017535932.py:3: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '1050.1875' has dtype incompatible with int64, please explicitly cast to a compatible dtype first.\n",
      "  leak_data_raw.iloc[i,-1] =  leak_data_raw.iloc[i, 2:-1].mean()\n"
     ]
    }
   ],
   "source": [
    "leak_data_raw['sensor_avg'] = 0\n",
    "for i in range(len(leak_data_raw)):\n",
    "    leak_data_raw.iloc[i,-1] =  leak_data_raw.iloc[i, 2:-1].mean()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Merging Data Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Leak and weather data merge\n",
    "# leak_data_raw['tTime'] = leak_data_raw['Time']\n",
    "# leak_data_raw['tTime'] = pd.to_datetime(leak_data_raw['tTime'], unit='s')\n",
    "# weather_df['tTime'] = pd.to_datetime(weather_df['tTime'], unit='s')\n",
    "\n",
    "# leak_data_raw['tTime'] = leak_data_raw['tTime'].dt.floor('min').dt.round('min')\n",
    "# df = pd.merge(leak_data_raw, weather_df, on='tTime', how='left')\n",
    "# df = df.drop(['temp', 'tTime'], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalizing leak intensity\n",
    "# df['leak_amount'] = 0\n",
    "# for index, row in leak_rate_df.iterrows():\n",
    "#     in_range = df['Time'].between(row['tStart'], row['tEnd'], inclusive='both')\n",
    "#     df.loc[in_range, 'leak_amount'] = np.array((row['LeakRate'] * (row['Duration'])), dtype=np.int32)\n",
    "# df['leak_amount_norm'] = (df['leak_amount'] - df['leak_amount'].min()) / (df['leak_amount'].max() - df['leak_amount'].min())\n",
    "\n",
    "\n",
    "# # Final prep\n",
    "# df = df.drop(['leak_amount'],axis=1)\n",
    "# df = df.astype(dtype='float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = leak_data_raw.drop(['temp','Time'], axis=1)\n",
    "# df = df.astype(dtype='float32')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import torch\n",
    "\n",
    "import lightning as L\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import model as m\n",
    "\n",
    "\n",
    "model = m.LightningLSTM()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "sensor1_df_normalized = (df['Sensor 1'] - df['Sensor 1'].min()) / (df['Sensor 1'].max() - df['Sensor 1'].min())\n",
    "sensor1_df_normalized = sensor1_df_normalized.astype(dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0     0.315134\n",
       "1     0.318008\n",
       "2     0.307471\n",
       "3     0.318966\n",
       "4     0.322318\n",
       "5     0.325671\n",
       "6     0.347701\n",
       "7     0.342433\n",
       "8     0.337165\n",
       "9     0.321839\n",
       "10    0.330460\n",
       "11    0.330460\n",
       "12    0.330460\n",
       "13    0.311303\n",
       "14    0.301245\n",
       "15    0.291188\n",
       "16    0.267241\n",
       "17    0.278257\n",
       "18    0.289272\n",
       "19    0.285441\n",
       "20    0.281609\n",
       "21    0.267241\n",
       "22    0.268199\n",
       "23    0.274904\n",
       "24    0.281609\n",
       "25    0.288314\n",
       "26    0.295019\n",
       "27    0.301724\n",
       "28    0.297893\n",
       "29    0.326628\n",
       "30    0.329502\n",
       "31    0.332375\n",
       "32    0.338123\n",
       "33    0.338123\n",
       "34    0.319923\n",
       "35    0.306513\n",
       "36    0.293103\n",
       "37    0.289272\n",
       "38    0.280651\n",
       "39    0.280651\n",
       "40    0.280651\n",
       "41    0.294061\n",
       "42    0.297893\n",
       "43    0.301724\n",
       "44    0.318966\n",
       "45    0.313218\n",
       "46    0.307471\n",
       "47    0.295977\n",
       "48    0.274425\n",
       "49    0.252874\n",
       "50    0.226054\n",
       "51    0.225096\n",
       "52    0.222222\n",
       "53    0.219349\n",
       "54    0.219828\n",
       "55    0.220307\n",
       "56    0.222222\n",
       "57    0.230843\n",
       "58    0.232759\n",
       "59    0.234674\n",
       "Name: Sensor 1, dtype: float32"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sensor1_df_normalized.iloc[0:60, ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "window_size = 60\n",
    "\n",
    "# Create a list of input sequences\n",
    "inputs = [sensor1_df_normalized.iloc[i:i+window_size].values for i in range(len(sensor1_df_normalized) - window_size)]\n",
    "\n",
    "# Create a list of corresponding labels\n",
    "labels = sensor1_df_normalized.iloc[window_size:].values\n",
    "\n",
    "inputs = torch.tensor(np.array(inputs), dtype=torch.float32) # convert to numpy\n",
    "labels = torch.tensor(np.array(labels), dtype=torch.float32)\n",
    "\n",
    "dataset =  TensorDataset(inputs, labels) #fix dimension\n",
    "dataloader = DataLoader(dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name | Type | Params\n",
      "------------------------------\n",
      "0 | lstm | LSTM | 16    \n",
      "------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "/opt/homebrew/lib/python3.9/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0:   1%|          | 471/83353 [00:03<09:12, 149.88it/s, v_num=24]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/lib/python3.9/site-packages/lightning/pytorch/trainer/call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "trainer = L.Trainer(max_epochs=5)\n",
    "trainer.fit(model,train_dataloaders=dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = pd.read_csv(\"Gasleak Data Sets/validation_files/sensor_readings.csv\")\n",
    "test_data.columns = [\"temp\",\"Time\",\"Sensor 1\", \"Sensor 2\", \"Sensor 3\", \"Sensor 4\", \"Sensor 5\", \"Sensor 6\", \"Sensor 7\", \"Sensor 8\", \"Sensor 9\", \"Sensor 10\", \"Sensor 11\", \"Sensor 12\", \"Sensor 13\", \"Sensor 14\", \"Sensor 15\", \"Sensor 16\", \"Sensor 17\", \"Sensor 18\", \"Sensor 19\", \"Sensor 20\", \"Sensor 21\", \"Sensor 22\", \"Sensor 23\", \"Sensor 24\"]\n",
    "sensor1_test_normalized = (test_data['Sensor 1'] - test_data['Sensor 1'].min()) / (test_data['Sensor 1'].max() - test_data['Sensor 1'].min())\n",
    "sensor1_test_normalized = sensor1_test_normalized.astype(dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set the window size\n",
    "window_size = 60\n",
    "\n",
    "# Create a list of input sequences\n",
    "inputs = [sensor1_test_normalized.iloc[i:i+window_size].values for i in range(len(sensor1_test_normalized) - window_size)]\n",
    "\n",
    "# Create a list of corresponding labels\n",
    "labels = sensor1_test_normalized.iloc[window_size:].values\n",
    "\n",
    "# # Convert the lists to pandas objects\n",
    "prediction_df = pd.concat([pd.DataFrame(inputs), pd.Series(labels)], axis=1)\n",
    "\n",
    "test_inputs = torch.tensor(np.array(inputs), dtype=torch.float32) # convert to numpy\n",
    "test_labels = torch.tensor(np.array(labels), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83728, 61)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([484.8488], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_normalized = model(test_inputs[0])\n",
    "prediction = prediction_normalized * (test_data['Sensor 1'].max() - test_data['Sensor 1'].min()) + test_data['Sensor 1'].min()\n",
    "\n",
    "# predictions = prediction_df.iloc[57:, :-1].apply(lambda x: model(torch.tensor(x, dtype=torch.float32)), axis=1)\n",
    "prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "shape '[1, 1]' is invalid for input of size 60",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 15\u001b[0m\n\u001b[1;32m     12\u001b[0m input_seq \u001b[39m=\u001b[39m input_seq\u001b[39m.\u001b[39mview(\u001b[39m1\u001b[39m, \u001b[39m60\u001b[39m)\n\u001b[1;32m     14\u001b[0m \u001b[39m# Pass the input sequence through the model\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m prediction \u001b[39m=\u001b[39m model(input_seq)\n\u001b[1;32m     17\u001b[0m \u001b[39m# Append the prediction to the list\u001b[39;00m\n\u001b[1;32m     18\u001b[0m predictions\u001b[39m.\u001b[39mappend(prediction)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1516\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_compiled_call_impl(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)  \u001b[39m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m-> 1518\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_impl(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m/opt/homebrew/lib/python3.9/site-packages/torch/nn/modules/module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1522\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1523\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1524\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1525\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1526\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1527\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1529\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1530\u001b[0m     result \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/Downloads/TSA Software Engenering/Data Science/model.py:11\u001b[0m, in \u001b[0;36mLightningLSTM.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m):\n\u001b[0;32m---> 11\u001b[0m     input_trans \u001b[39m=\u001b[39m \u001b[39minput\u001b[39;49m\u001b[39m.\u001b[39;49mview(\u001b[39mlen\u001b[39;49m(\u001b[39minput\u001b[39;49m), \u001b[39m1\u001b[39;49m)\n\u001b[1;32m     12\u001b[0m     lstm_out, temp  \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mlstm(input_trans)\n\u001b[1;32m     14\u001b[0m     prediction \u001b[39m=\u001b[39m lstm_out[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n",
      "\u001b[0;31mRuntimeError\u001b[0m: shape '[1, 1]' is invalid for input of size 60"
     ]
    }
   ],
   "source": [
    "# Assuming prediction_df is a pandas DataFrame and model is a PyTorch model\n",
    "\n",
    "# Convert the input data to a PyTorch tensor\n",
    "inputs = torch.tensor(prediction_df.iloc[:, :-1].values, dtype=torch.float32)\n",
    "\n",
    "# Create a list to store the predictions\n",
    "predictions = []\n",
    "\n",
    "# Iterate over each input sequence\n",
    "for input_seq in inputs:\n",
    "    # Reshape the input sequence to (1, 60)\n",
    "    input_seq = input_seq.view(1, 60)\n",
    "    \n",
    "    # Pass the input sequence through the model\n",
    "    prediction = model(input_seq)\n",
    "    \n",
    "    # Append the prediction to the list\n",
    "    predictions.append(prediction)\n",
    "\n",
    "# Stack the predictions into a single tensor\n",
    "predictions = torch.cat(predictions, dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# path_to_best_checkpoint = trainer.checkpoint_callback.best_model_path\n",
    "# trainer = L.Trainer(max_epochs=4)\n",
    "# trainer.fit(model,train_dataloaders=dataloader, ckpt_path=path_to_best_checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(model(torch.tensor([848.0,850.0,855.0,854.0,855.0,853.0,852.5,852.0,859.0,855.0])).detach())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
